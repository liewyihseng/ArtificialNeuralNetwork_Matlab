{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea66be1d",
   "metadata": {},
   "source": [
    "## Classification using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7881f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import logging\n",
    "logging.basicConfig(level = logging.DEBUG)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d858c",
   "metadata": {},
   "source": [
    "###### ANN Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_model(n_in, n_out, n_h1 = 10, n_h2 = 10):\n",
    "    \n",
    "    #Network parameters\n",
    "    n_hidden1 = n_h1\n",
    "    n_hidden2 = n_h2\n",
    "    n_input = n_in\n",
    "    n_output = n_out\n",
    "    \n",
    "    #DEFINING WEIGHTS AND BIASES\n",
    "    #Biases first hidden layer\n",
    "    b1 = tf.Variable(tf.random_normal([n_hidden1]))\n",
    "    #Biases second hidden layer\n",
    "    b2 = tf.Variable(tf.random_normal([n_hidden2]))\n",
    "    #Biases output layer\n",
    "    b3 = tf.Variable(tf.random_normal([n_output]))\n",
    "    #Weights connecting input layer with first hidden layer\n",
    "    w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))\n",
    "    #Weights connecting first hidden layer with second hidden layer\n",
    "    w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]))\n",
    "    #Weights connecting second hidden layer with output layer\n",
    "    w3 = tf.Variable(tf.random_normal([n_hidden2, n_output]))\n",
    "    \n",
    "    def multilayer_perceptron(input_d):\n",
    "        #Task of neurons of first hidden layer\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(input_d, w1), b1))\n",
    "        #Task of neurons of second hidden layer\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2), b2))\n",
    "        #Task of neurons of output layer\n",
    "        out_layer = tf.add(tf.matmul(layer_2, w3),b3)\n",
    "\n",
    "        return out_layer\n",
    "    \n",
    "    neural_network = multilayer_perceptron(X)\n",
    "    \n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc53972",
   "metadata": {},
   "source": [
    "###### ANN Classifier Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d786d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, n_epoch = 1000, batch_size = 1000):\n",
    "    \n",
    "    #Learning parmeters\n",
    "    number_epoch = n_epoch\n",
    "    \n",
    "    #Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        total_batch = int(len(train_y) / batch_size)\n",
    "        \n",
    "        #Training epoch\n",
    "        for epoch in range(number_epoch):\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                batch_x = train_x[i * batch_size:min(i * batch_size + batch_size, len(train_x)), :]\n",
    "                batch_y = train_y[i * batch_size:min(i * batch_size + batch_size, len(train_y)), :]\n",
    "                sess.run(optimizer, feed_dict={X: batch_x, Y:batch_y})\n",
    "            \n",
    "            #Display the epoch\n",
    "            #if epoch % 100 == 0:\n",
    "            #    print(\"Epoch:\", '%d' % (epoch))\n",
    "                  \n",
    "        pred = (neural_network)\n",
    "        \n",
    "        #Class estimation using argmax\n",
    "        estimated_class = tf.argmax(pred, 1).eval({X: test_x})\n",
    "        correct_prediction = np.equal(estimated_class, test_y_label)\n",
    "        result = np.column_stack((test_y_label, estimated_class, correct_prediction))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022a32f",
   "metadata": {},
   "source": [
    "###### Peformance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8364094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_evaluation(result):\n",
    "    \n",
    "    #select data with predicted label '1'\n",
    "    tt_idx = np.where(result[:, 1] == 1);\n",
    "    #select data with predicted label '0'\n",
    "    tf_idx = np.where(result[:, 1] == 0);\n",
    "\n",
    "    #merge data with predicted label '1'\n",
    "    testing_true = result[tt_idx]\n",
    "    #merge data with predicted label '1'\n",
    "    testing_false = result[tf_idx]\n",
    "\n",
    "    #calculate number of true positive result\n",
    "    t_positive = sum(testing_true[:,2] == 1)\n",
    "    #calculate number of false positive result\n",
    "    f_positive = sum(testing_true[:,2] == 0)\n",
    "\n",
    "    \n",
    "    #calculate number of true negative result\n",
    "    t_negative = sum(testing_false[:,2] == 1)\n",
    "    #calculate number of false negative result\n",
    "    f_negative = sum(testing_false[:,2] == 0)\n",
    "\n",
    "    #calcaulte precision value of the result\n",
    "    precision = t_positive / (t_positive + f_positive)\n",
    "    #calcaulte recall value of the result\n",
    "    recall = t_positive / (t_positive + f_negative)\n",
    "    #calcaulte f-measure value of the result\n",
    "    fmeasure = 2 * ((precision * recall) / (precision + recall))\n",
    "    #calcaulte accuracy of the result\n",
    "    accuracy = (t_positive + t_negative) / result.shape[0]\n",
    "\n",
    "    return [precision, recall, fmeasure, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b9329",
   "metadata": {},
   "source": [
    "###### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3240632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    \n",
    "    #binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    feature = data.reshape(len(data), 1)   \n",
    "    return onehot_encoder.fit_transform(feature)\n",
    "\n",
    "def data_cleaning(data, column, invalid_value):\n",
    "    \n",
    "    #search data with invalid feature value\n",
    "    idx = np.isin(data[:, column], invalid_value)\n",
    "    invalid_idx = np.nonzero(idx)\n",
    "    #remove data with invalid feature value\n",
    "    return np.delete(data, invalid_idx, 0)\n",
    "\n",
    "#load dataset\n",
    "data=np.genfromtxt('datasets/default of credit card clients.csv', delimiter=',')\n",
    "\n",
    "#remove first row of dataset (feature name)\n",
    "data = np.delete(data, 0, 0)\n",
    "#remove first column of dataset (data index)\n",
    "data = np.delete(data, 0, 1)\n",
    "\n",
    "data = data_cleaning(data, 2, [0,5,6])\n",
    "data = data_cleaning(data, 3, 0)\n",
    "\n",
    "#normalise continous data using z-score function\n",
    "normalised_continous_data = zscore(data[:, [0,4]+list(range(11,23))])\n",
    "\n",
    "#perform one hot encoding on categorical features\n",
    "gender_one_hot = one_hot_encoding(data[:,1])\n",
    "education_one_hot = one_hot_encoding(data[:,2])\n",
    "marital_one_hot = one_hot_encoding(data[:,3])\n",
    "label_one_hot = one_hot_encoding(data[:,23])\n",
    "\n",
    "#merge dataset after preprocessing\n",
    "classification_data = np.column_stack((normalised_continous_data, gender_one_hot,education_one_hot,marital_one_hot, data[:,5:11], label_one_hot))\n",
    "\n",
    "#sort dataset according to label\n",
    "sorted_classification_data = classification_data[np.argsort(classification_data[:, 30])]\n",
    "\n",
    "#calculate dataset with label '0'\n",
    "numof0 = sum(sorted_classification_data[:, 29] == 1);\n",
    "#calculate dataset with label '1'\n",
    "numof1 = sorted_classification_data.shape[0]-numof0\n",
    "\n",
    "#suffle dataset with label '0'\n",
    "np.random.shuffle(sorted_classification_data[0:numof1-1,:])\n",
    "#suffle dataset with label '1'\n",
    "np.random.shuffle(sorted_classification_data[numof0:-1,:])\n",
    "\n",
    "#balancing dataset with label '0'\n",
    "dataset_label_0 = sorted_classification_data[0: numof1-1, :];\n",
    "#balancing dataset with label '1'\n",
    "dataset_label_1 = sorted_classification_data[sorted_classification_data.shape[0]-numof1:-1, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed361ca",
   "metadata": {},
   "source": [
    "###### Optimizer Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(neural_network, learning_c = 0.2):\n",
    "    \n",
    "    #Learning parmeters\n",
    "    learning_constant = learning_c\n",
    "    \n",
    "    #Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "     \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba79b51",
   "metadata": {},
   "source": [
    "###### Functions used for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate initial offspring for 1st generation\n",
    "def generate_initial_offspring(decimal_flag, lower_limit, upper_limit):\n",
    "    \n",
    "    #check if generated offspring need to be in floats\n",
    "    if decimal_flag:\n",
    "        offspring = np.round(np.random.uniform(lower_limit, upper_limit, 10), decimals = 3)\n",
    "    else:\n",
    "        X = np.round(np.random.uniform(lower_limit, upper_limit, 10))\n",
    "        offspring = [int(item) for item in X]\n",
    "        \n",
    "    return offspring\n",
    "\n",
    "# generate offspring from previous generation\n",
    "def generate_offspring_np(best_parent, decimal_flag, lower_limit,upper_limit):\n",
    "    \n",
    "    #calculate range of the parent's generation\n",
    "    std_deviation = (upper_limit - lower_limit) * 0.1\n",
    "    \n",
    "    #check if generated offspring need to be in floats\n",
    "    if decimal_flag:\n",
    "        X = np.round(np.random.normal(best_parent, std_deviation, size=(1, 10)),decimals = 3)\n",
    "        offspring = [best_parent if (x < lower_limit or x > upper_limit) else x for x in X[0]]\n",
    "    else:\n",
    "        X = np.round(np.random.normal(best_parent, std_deviation, size=(1, 10)))\n",
    "        X = [int(item) for item in X[0]]\n",
    "        offspring = [best_parent if (x < lower_limit or x > upper_limit) else x for x in X]\n",
    "    \n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef39e2",
   "metadata": {},
   "source": [
    "###### Hyperparameter Tuning on the loss function of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495193c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack balance dataset with abel '0' and '1'\n",
    "balanced_classification_data = np.vstack((dataset_label_0, dataset_label_1))\n",
    "#shuffle the balanced dataset\n",
    "np.random.shuffle(balanced_classification_data)\n",
    "\n",
    "#split shuffled dataset for tuning and test set\n",
    "test_dataset_size = math.floor(balanced_classification_data.shape[0] / 10)\n",
    "tuning_dataset_size = math.floor((balanced_classification_data.shape[0] - test_dataset_size) / 2)\n",
    "\n",
    "#spliting feature and label for test set \n",
    "test_x = balanced_classification_data[0 : test_dataset_size, :29]\n",
    "test_y = balanced_classification_data[0 : test_dataset_size, 29:]\n",
    "test_y_label = test_y[:, 1]\n",
    "\n",
    "#spliting feature and label for first tuning set\n",
    "tuning1_x = balanced_classification_data[test_dataset_size : test_dataset_size + tuning_dataset_size, :29]\n",
    "tuning1_y = balanced_classification_data[test_dataset_size : test_dataset_size + tuning_dataset_size, 29:]\n",
    "tuning1_y_label = tuning1_y[:, 1]\n",
    "\n",
    "#spliting feature and label for second tuning set\n",
    "tuning2_x = balanced_classification_data[test_dataset_size + tuning_dataset_size : -1, :29]\n",
    "tuning2_y = balanced_classification_data[test_dataset_size + tuning_dataset_size : -1, 29:]\n",
    "tuning2_y_label = tuning2_y[:, 1]\n",
    "\n",
    "#stack tuning dataset\n",
    "train_x = np.vstack((tuning1_x, tuning2_x))\n",
    "train_y = np.vstack((tuning1_y, tuning2_y))\n",
    "\n",
    "#Network parameters\n",
    "n_input = 29\n",
    "n_output = 2\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 10\n",
    "\n",
    "#Learning parmeters\n",
    "learning_constant = 0.2\n",
    "number_epoch = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "#Defining the input and the output\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "\n",
    "#set the loss function to be mean absolute error\n",
    "loss_op = tf.keras.losses.MeanAbsoluteError()(neural_network, Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "print(\"Mean Absolute Error: \", fmeasure)\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "\n",
    "#set the loss function to be mean absolute percentage error\n",
    "loss_op = tf.keras.losses.MeanAbsolutePercentageError()(neural_network, Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "#display the performance of the model\n",
    "print(\"Mean Absolute Percentage Error: \", fmeasure)\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "\n",
    "#set the loss function to be mean squared logarithmic error\n",
    "loss_op = tf.keras.losses.MeanSquaredLogarithmicError()(neural_network, Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "#display the performance of the model\n",
    "print(\"Mean Squared Logarithmic Error: \", fmeasure)\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "\n",
    "#set the loss function to be softmax cross entropy error\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "#display the performance of the model\n",
    "print(\"Softmax Cross Entropy: \", fmeasure)\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "\n",
    "#set the loss function to be binary cross entropy error\n",
    "loss_op = tf.keras.losses.BinaryCrossentropy()(neural_network, Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "#display the performance of the model\n",
    "print(\"Binary Cross Entropy: \", fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728181b",
   "metadata": {},
   "source": [
    "###### Hyperparameter Tuning on the optimizer function of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "print(\"Gradient Descent Optimizer: \", fmeasure)\n",
    "\n",
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_constant).minimize(loss_op)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "print(\"ADAM Optimizer: \", fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85759e",
   "metadata": {},
   "source": [
    "###### Hyperparameter Tuning on network topology of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e89493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "n_input = 29\n",
    "n_output = 2\n",
    "\n",
    "#Learning parmeters\n",
    "learning_constant = 0.05\n",
    "number_epoch = 1000\n",
    "batch_size = 50\n",
    "\n",
    "#Defining the input and the output\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "#define the output variable of tuning (x1: number of node in first hidden layer; x2: number of node in second hidden layer;z1: f-measure of the result)\n",
    "x1 = np.empty((0, 1))\n",
    "y1 = np.empty((0, 1))\n",
    "z1 = np.empty((0, 1))\n",
    "\n",
    "best_n_hidden = [0, 0, 0]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    print(\"generation: \", i + 1)                                                              \n",
    "    print(\"Best so far:\", best_n_hidden)\n",
    "    \n",
    "    # check the number of generation and generate offspring\n",
    "    if i == 0:\n",
    "        n_hidden_offspring = np.vstack((generate_initial_offspring(False, n_output, n_input), generate_initial_offspring(False, n_output, n_input))).T\n",
    "    else:\n",
    "        n_hidden_offspring = np.vstack((generate_offspring_np(best_n_hidden[0], False, n_output, n_input), generate_offspring_np(best_n_hidden[1], False, n_output, n_input))).T\n",
    "    \n",
    "    # evaluate the performance of the model based on the value of offspring\n",
    "    for n_hidden in n_hidden_offspring:\n",
    "        \n",
    "        print(\"n_hidden1: \", n_hidden[0], \"n_hidden2:\", n_hidden[1])\n",
    "        \n",
    "        neural_network = create_ann_model(n_input, n_output, n_hidden[0], n_hidden[1])\n",
    "        optimizer = create_optimizer(neural_network , learning_constant)\n",
    "        result1 = train_test_ann_classifier(neural_network, optimizer, tuning1_x, tuning1_y, tuning2_x, tuning2_y_label, number_epoch, batch_size)\n",
    "        \n",
    "        #interchange training set with validation set\n",
    "        neural_network = create_ann_model(n_input, n_output, n_hidden[0], n_hidden[1])\n",
    "        optimizer = create_optimizer(neural_network , learning_constant)\n",
    "        result2 = train_test_ann_classifier(neural_network, optimizer, tuning2_x, tuning2_y, tuning1_x, tuning1_y_label, number_epoch, batch_size)\n",
    "        \n",
    "        result = np.vstack((result1, result2))\n",
    "       \n",
    "        [precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "        \n",
    "        x1 = np.vstack((x1, n_hidden[0]))\n",
    "        y1 = np.vstack((y1, n_hidden[1]))\n",
    "        z1 = np.vstack((z1, fmeasure))\n",
    "        \n",
    "        #replace the best parents with the offpsring with highest f-measure\n",
    "        if fmeasure >= best_n_hidden[2]:\n",
    "            best_n_hidden = [n_hidden[0], n_hidden[1], fmeasure]\n",
    "\n",
    "n_hidden1 = best_n_hidden[0]\n",
    "n_hidden2 = best_n_hidden[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba02725",
   "metadata": {},
   "source": [
    "###### scatter plot on the hyperparameter on the network topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x1, y1, z1)\n",
    "ax.view_init(10, 10)\n",
    "plt.show()\n",
    "\n",
    "print(best_n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a1135",
   "metadata": {},
   "source": [
    "###### hyperparameter tuning on the learning rate, batch size and epoch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.empty((0, 1))\n",
    "y2 = np.empty((0, 1))\n",
    "anotations = np.empty((0, 3))\n",
    "\n",
    "#[learning_constant, number_epoch, fmeasure]\n",
    "best_learning_params = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    print(\"generation: \", i + 1)                                                              \n",
    "    print(\"Best so far:\", best_learning_params)\n",
    "    \n",
    "    # check the iteration of generation and generate offspring\n",
    "    if i == 0:        \n",
    "        learning_rate_offspring = generate_initial_offspring(True, 0, 1)\n",
    "        number_epoch_offspring = generate_initial_offspring(False, 1, 5000)\n",
    "        batch_size_offspring = generate_initial_offspring(False, 1, 5000)\n",
    "        \n",
    "    else:\n",
    "        learning_rate_offspring = generate_offspring_np(best_learning_params[0], True, 0, 1)\n",
    "        number_epoch_offspring = generate_offspring_np(best_learning_params[1], False, 1, 5000)\n",
    "        batch_size_offspring = generate_offspring_np(best_learning_params[2], False, 1, 5000)\n",
    "        \n",
    "    learning_params_offspring = np.vstack((learning_rate_offspring, number_epoch_offspring, batch_size_offspring)).T\n",
    "\n",
    "    # evaluate the performance of the model based on the value of offspring    \n",
    "    for learning_params in learning_params_offspring:\n",
    "        \n",
    "        print(\"Learning Constant: \", learning_params[0], \"Number Epoch:\", learning_params[1], \"Batch Size\", learning_params[2])\n",
    "        \n",
    "        neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "        optimizer = create_optimizer(neural_network , learning_params[0])\n",
    "        result1 = train_test_ann_classifier(neural_network, optimizer, tuning1_x, tuning1_y, tuning2_x, tuning2_y_label, int(learning_params[1]), int(learning_params[2]))\n",
    "        \n",
    "        #interchange training set with validation set\n",
    "        neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "        optimizer = create_optimizer(neural_network , learning_params[0])\n",
    "        result2 = train_test_ann_classifier(neural_network, optimizer, tuning2_x, tuning2_y, tuning1_x, tuning1_y_label, int(learning_params[1]), int(learning_params[2]))\n",
    "        \n",
    "        result = np.vstack((result1, result2))\n",
    "       \n",
    "        [precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "        \n",
    "        x2 = np.vstack((x2, i + 1))\n",
    "        y2 = np.vstack((y2, fmeasure))\n",
    "        anotations = np.vstack((anotations, learning_params))\n",
    "        \n",
    "        #replace the best parents with the offpsring with highest f-measure\n",
    "        if fmeasure >= best_learning_params[3]:\n",
    "            best_learning_params = [learning_params[0], learning_params[1], learning_params[2], fmeasure]\n",
    "\n",
    "learning_constant = best_learning_params[0]\n",
    "number_epoch = int(best_learning_params[1])\n",
    "batch_size = int(best_learning_params[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c35bfc",
   "metadata": {},
   "source": [
    "###### scatter plot on the hyperparameter on the learning rate, batch size and epoch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x2, y2)\n",
    "print(best_learning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20aab8",
   "metadata": {},
   "source": [
    "###### evaluate the performance of the tuned model using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb47ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "optimizer = create_optimizer(neural_network , learning_constant)\n",
    "result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "print([precision, recall, fmeasure, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739a88d",
   "metadata": {},
   "source": [
    "###### 10-fold cross validation using the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d31b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the size of dataset in each partition\n",
    "partition_size = math.floor(dataset_label_0.shape[0] / 10)\n",
    "\n",
    "#define output result\n",
    "result = np.empty((0, 3), int)\n",
    "    \n",
    "#Network parameters\n",
    "#n_hidden1 = 20\n",
    "#n_hidden2 = 15\n",
    "n_input = 29\n",
    "n_output = 2\n",
    "#Learning parmeters\n",
    "#learning_constant = 0.01\n",
    "#number_epoch = 1000\n",
    "#batch_size = 300\n",
    "\n",
    "#Defining the input and the output\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "for f in range(10):\n",
    "    \n",
    "    #maintain the blance of dataset in each partision\n",
    "    test_0_x = dataset_label_0[f * partition_size : (f + 1) * partition_size, 0:29]\n",
    "    test_0_y = dataset_label_0[f * partition_size : (f + 1) * partition_size,29:31]\n",
    "    \n",
    "    test_1_x = dataset_label_1[f * partition_size : (f + 1) * partition_size, 0:29]\n",
    "    test_1_y = dataset_label_1[f * partition_size : (f + 1) * partition_size,29:31]\n",
    "\n",
    "    train_0_set = np.delete(dataset_label_0, np.arange(f * partition_size, (f + 1) * partition_size), 0)\n",
    "    train_1_set = np.delete(dataset_label_1, np.arange(f * partition_size, (f + 1) * partition_size), 0)\n",
    "    \n",
    "    concat_train = np.vstack((train_0_set,train_1_set))\n",
    "    np.random.shuffle(concat_train)\n",
    "\n",
    "    train_x = concat_train[:,:-2]\n",
    "    train_y = concat_train[:,-2:]\n",
    "\n",
    "    test_x = np.vstack((test_0_x, test_1_x))\n",
    "    test_y = np.vstack((test_0_y, test_1_y))\n",
    "    test_y_label = test_y[:, 1]\n",
    "    \n",
    "    #define the ANN model\n",
    "    neural_network = create_ann_model(n_input, n_output, n_hidden1, n_hidden2)\n",
    "    optimizer = create_optimizer(neural_network , learning_constant)\n",
    "    \n",
    "    #train the model\n",
    "    fold_result = train_test_ann_classifier(neural_network, optimizer, train_x, train_y, test_x, test_y_label, number_epoch)\n",
    "    \n",
    "    #evaluate the result on the fold\n",
    "    fold_evaluation_result = result_evaluation(fold_result)\n",
    "    \n",
    "    #print the evaluation result in each fold\n",
    "    print(fold_evaluation_result)\n",
    "    \n",
    "    #concatenate the result\n",
    "    result = np.vstack((result, fold_result))\n",
    "\n",
    "#evaluate the average performance of the model in all fold\n",
    "[precision, recall, fmeasure, accuracy] = result_evaluation(result)\n",
    "\n",
    "#display the average performace of the model in all fold\n",
    "print([precision, recall, fmeasure, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d8f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
